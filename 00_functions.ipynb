{
 "cells": [
  {
   "cell_type": "raw",
   "id": "50aed95a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"00_functions\"\n",
    "author: \"JTK\"\n",
    "date: \"2025-05-14\"\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d608999",
   "metadata": {
    "lines_to_next_cell": 0,
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5dd66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fc4ad91",
   "metadata": {},
   "source": [
    "### The selector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03881a5c",
   "metadata": {
    "name": "lgbm-selector"
   },
   "outputs": [],
   "source": [
    "\n",
    "lgbm_selector <- function(constituent_df,\n",
    "                          selector = \"shap\",\n",
    "                          conc_scalars = NULL\n",
    "                          \n",
    "                          ) {\n",
    "  \n",
    "    \n",
    "  \n",
    "\n",
    "      model_stats_lgbm <- list()\n",
    "      \n",
    "      var_imp <- list()\n",
    "      \n",
    "      vars <- list()\n",
    "      \n",
    "      all_model_stats <- list()\n",
    "      \n",
    "      shap_values <- list()\n",
    "      \n",
    "      shap_value_summary <- list()\n",
    "      \n",
    "      model_returns <- list()\n",
    "      \n",
    "      predicted_observed_ts <- list()\n",
    "      \n",
    "      \n",
    "      ### Declare the predictors\n",
    "      \n",
    "        \n",
    "      predictors <- constituent_df\n",
    "  \n",
    "      \n",
    "      for(i in 1:ncol(predictors)) {\n",
    "        \n",
    "        \n",
    "        \n",
    "          run <- paste0(\"run\", i)\n",
    "        \n",
    "                    \n",
    "          cat(crayon::yellow(\"\\nModeling Run\", i, \"\\n\"))\n",
    "\n",
    "\n",
    "                \n",
    "                  for(j in c(2003, 2008, 2013, 2018)) {\n",
    "                \n",
    "              \n",
    "                        test_years <- seq(j-4, j, 1)\n",
    "                        \n",
    "                        train_years <- seq(1990, j-5, 1)\n",
    "                        \n",
    "                        k <- (2018 - j)/5\n",
    "                  \n",
    "                        k <- ifelse(k == 0, 1, k+1)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        cat(crayon::cyan(\"\\nTraining \", first(train_years), \":\",\n",
    "                                           last(train_years),\n",
    "                                           \"\\n\")) \n",
    "                        cat(crayon::magenta(\"\\nTesting \", first(test_years), \":\",\n",
    "                                           last(test_years),\n",
    "                                           \"\\n\")) \n",
    "                        \n",
    "                            \n",
    "                        \n",
    "                        #### Training data\n",
    "                            predictor_dataset_train <- predictors %>%\n",
    "                            filter(wateryear %in% train_years)\n",
    "                            \n",
    "                            if(nrow(predictor_dataset_train) < 1) next\n",
    "                     \n",
    "                          ### Now subset the testing data to that same subset\n",
    "                          predictor_dataset_test <- predictors %>%\n",
    "                            filter(wateryear %in% test_years) %>%\n",
    "                            dplyr::select(colnames(predictor_dataset_train))\n",
    "                          \n",
    "                          #######################\n",
    "                        \n",
    "                          ### Declare the predictor and response variables \n",
    "                        preds <- data.matrix(predictor_dataset_train %>%\n",
    "                                                    dplyr::select(!c(log_conc,\n",
    "                                                                     wateryear,\n",
    "                                                                     date, \n",
    "                                                                     tributary)))\n",
    "                        \n",
    "                        response <- predictor_dataset_train$log_conc\n",
    "                        \n",
    "                        ### Set up the environment - \n",
    "                        #### this is just preparing the dataset API to be used by lightgbm. \n",
    "                        #### This is our training data\n",
    "                        train_lgbm <- lgb.Dataset(preds, \n",
    "                                                         label = response,\n",
    "\n",
    "                                                         ) \n",
    "                        \n",
    "                        ### Declare the test data\n",
    "                        test_lgbm <- data.matrix(predictor_dataset_test %>%\n",
    "                                                              dplyr::select(!c(log_conc,\n",
    "                                                                     wateryear,\n",
    "                                                                     date, \n",
    "                                                                     tributary)))\n",
    "                    \n",
    "\n",
    "                        \n",
    "                        #########\n",
    "                    \n",
    "                      ### Declare the hyperparameters \n",
    "                      ### These are just default for now\n",
    "                      \n",
    "                      hyperparams <- list(objective = \"regression\",\n",
    "                                          num_leaves = 31L,\n",
    "                                          learning_rate = 0.1,\n",
    "                                          min_data_in_leaf = 20L,\n",
    "                                          num_threads = 10L)\n",
    "\n",
    "                        \n",
    "                        ### Train the model\n",
    "                        \n",
    "                        set.seed(913)\n",
    "                        \n",
    "                        nutrient_model_lgbm <- lgb.train(hyperparams,\n",
    "                                                          data = train_lgbm,\n",
    "                                                          verbose = 1L,\n",
    "                                                          nrounds = 100L\n",
    "                                                         )\n",
    "                        \n",
    "                        ### Get model fits on training data\n",
    "                        nutrient_fits <- predict(nutrient_model_lgbm, \n",
    "                                                       data = preds) %>%\n",
    "                          as_tibble() %>% rename(log_predicted_conc = 1)\n",
    "                        \n",
    "                        \n",
    "                        ### Predict with the model on test data\n",
    "                        nutrient_predicted <- predict(nutrient_model_lgbm, \n",
    "                                                       data = test_lgbm) %>%\n",
    "                          as_tibble() %>% rename(log_predicted_conc = 1)\n",
    "                        \n",
    "                        ### Calculate the SHAP values\n",
    "                        shap_values[[k]] <- SHAPforxgboost::shap.prep(xgb_model = \n",
    "                                                                        nutrient_model_lgbm, \n",
    "                                                                   X_train = test_lgbm)\n",
    "                        \n",
    "                        shap_value_summary[[j]] <- shap_values[[k]] %>%\n",
    "                              as_tibble() %>%\n",
    "                              dplyr::group_by(variable) %>%\n",
    "                              summarise(sd_shap = sd(value),\n",
    "                                        feature_importance = mean_value[1]) %>%\n",
    "                              mutate(sd_plus_imp = sd_shap + feature_importance)\n",
    "                        \n",
    "                        \n",
    "                        ### Bind predictions on test data\n",
    "                        ### to observatios of test data\n",
    "                        predicted_observed <- bind_cols(predictor_dataset_test %>%\n",
    "                                                                dplyr::rename(log_observed_conc =\n",
    "                                                                                log_conc),\n",
    "                                                                       nutrient_predicted) \n",
    "                        \n",
    "### Now use the smearing coefficient to convert back to non-log\n",
    "predicted_observed_resc <- predicted_observed %>%\n",
    "  mutate(observed_conc = 10^(log_observed_conc*conc_scalars$sd + conc_scalars$mean),\n",
    "         predicted_conc = 10^(log_predicted_conc*conc_scalars$sd + conc_scalars$mean))\n",
    "                        \n",
    "                        predicted_observed_ts[[j]] <- predicted_observed_resc %>%\n",
    "                          dplyr::select(tributary, date, predicted_conc, observed_conc)\n",
    "                        \n",
    "                    \n",
    "                        #### Evaluate\n",
    "                        #### For each watershed\n",
    "\n",
    "model_stats_simple <- predicted_observed_resc %>%\n",
    "  ungroup() %>%\n",
    "  dplyr::group_by(tributary) %>%\n",
    "  summarise(mae = hydroGOF::mae(predicted_conc, observed_conc),\n",
    "            nse = hydroGOF::NSE(predicted_conc, observed_conc),\n",
    "            kge = hydroGOF::KGE(predicted_conc, \n",
    "                                observed_conc),\n",
    "            pbias = hydroGOF::pbias(predicted_conc,\n",
    "                                    observed_conc)) %>%\n",
    "  dplyr::ungroup() \n",
    "\n",
    "#### Median across all \n",
    "\n",
    "model_stats_lgbm[[j]] <- model_stats_simple %>%\n",
    "  reframe(across(where(is.numeric),\n",
    "                 list(median = ~median(.x)),\n",
    "                 .names = \"{.fn}_{.col}\"\n",
    "                 ))\n",
    "\n",
    "                        \n",
    "                        var_imp[[j]] <- lgb.importance(nutrient_model_lgbm , \n",
    "                                                                         percentage = TRUE)\n",
    "                        \n",
    "                        \n",
    "                  \n",
    "            #####################################################################\n",
    "          \n",
    "          \n",
    "          }\n",
    "          \n",
    "            \n",
    "            \n",
    "              all_var_imp <- bind_rows(var_imp) \n",
    "              \n",
    "              all_shap_value_summary <- bind_rows(shap_value_summary) %>%\n",
    "                dplyr::group_by(variable) %>%\n",
    "                summarise(mean_sd_plus_imp = mean(sd_plus_imp))\n",
    "          \n",
    "          all_model_stats[[i]] <- bind_rows(model_stats_lgbm) %>%\n",
    "            mutate(model = i)\n",
    "          \n",
    "          summary_var_imp <- all_var_imp %>%\n",
    "            dplyr::group_by(Feature) %>%\n",
    "            summarise(mean_Gain = mean(Gain)) %>%\n",
    "            dplyr::ungroup()\n",
    "          \n",
    "            if(selector == \"shap\") {\n",
    "              \n",
    "                one_removed_predictors <- all_shap_value_summary %>%\n",
    "                  dplyr::ungroup() %>%\n",
    "                  arrange(desc(mean_sd_plus_imp)) %>%\n",
    "                  dplyr::slice(-nrow(.))\n",
    "                \n",
    "          vars[[i]] <- all_shap_value_summary %>%\n",
    "            ungroup() %>%\n",
    "            mutate(model = i) %>%\n",
    "            rename(Feature = variable)\n",
    "          \n",
    "          ### See how many we have left\n",
    "          var_count <- length(one_removed_predictors$variable)\n",
    "          \n",
    "          if(var_count == 0) break \n",
    "          \n",
    "          \n",
    "            ### Update variable list\n",
    "          predictors <- predictors %>%\n",
    "            dplyr::select(one_removed_predictors$variable,\n",
    "                          log_conc,\n",
    "                          wateryear,\n",
    "                          date, \n",
    "                          tributary)\n",
    "          \n",
    "              \n",
    "            } else if(selector == \"gain\"){\n",
    "              \n",
    "                one_removed_predictors <- summary_var_imp %>%\n",
    "                  dplyr::ungroup() %>%\n",
    "                  arrange(desc(mean_Gain)) %>%\n",
    "                  dplyr::slice(-nrow(.))\n",
    "                \n",
    "                          vars[[i]] <- summary_var_imp %>%\n",
    "            ungroup() %>%\n",
    "            mutate(model = i)\n",
    "          \n",
    "          ### See how many we have left\n",
    "          var_count <- length(one_removed_predictors$Feature)\n",
    "          \n",
    "          if(var_count == 0) break \n",
    "          \n",
    "          \n",
    "            ### Update variable list\n",
    "          predictors <- predictors %>%\n",
    "            dplyr::select(one_removed_predictors$Feature,\n",
    "                          log_conc,\n",
    "                          wateryear,\n",
    "                          date, \n",
    "                          tributary)\n",
    "          \n",
    "            }\n",
    "          \n",
    "      }\n",
    "          \n",
    "        \n",
    "      \n",
    "      \n",
    "      all_all_model_stats <- bind_rows(all_model_stats)\n",
    "      \n",
    "      all_var_imp <- bind_rows(vars)\n",
    "      \n",
    "      \n",
    "      #### Save outputs\n",
    "      \n",
    " \n",
    "      \n",
    "      model_returns[[1]] <- all_var_imp\n",
    "      \n",
    "      #model_returns[[4]] <- shap_values\n",
    "      \n",
    "      \n",
    "      ## examine the model summary statistics\n",
    "      summary_model_stats <- all_all_model_stats %>%\n",
    "        ungroup()\n",
    "      \n",
    "      \n",
    "      collapsed_models <- all_var_imp %>%\n",
    "        group_by(model) %>%\n",
    "        arrange(Feature, .by_group = TRUE) %>%\n",
    "        summarise(all_vars = paste(Feature, collapse = \",\")) %>%\n",
    "        full_join(., summary_model_stats, \n",
    "                  by = \"model\")\n",
    "      \n",
    "        \n",
    "        model_stats <- collapsed_models %>%\n",
    "              dplyr::group_by(model, all_vars) %>%\n",
    "                  summarise(#n = n(),\n",
    "                            mean_kge = mean(median_kge, na.rm = TRUE),\n",
    "                                  mean_nse = mean(median_nse, na.rm = TRUE),\n",
    "                                  mean_mae = mean(median_mae, na.rm = TRUE),\n",
    "                                  mean_pbias = mean(median_pbias, na.rm = TRUE),\n",
    "                            sd_kge = sd(median_kge),\n",
    "                            sd_nse = sd(median_nse),\n",
    "                            sd_mae = sd(median_mae),\n",
    "                            sd_pbias = sd(median_pbias),\n",
    "                            ) \n",
    "            \n",
    "              model_returns[[2]] <- model_stats\n",
    "              \n",
    "        \n",
    "        \n",
    "      \n",
    "      \n",
    "      return(model_returns)\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d96338",
   "metadata": {},
   "source": [
    "## Plot variable selection stats\n",
    "\n",
    "This function takes a dataframe of model performance stats generated by the \n",
    "variable selection process and plots those as a function of  model \"number\",\n",
    "where model number 1 has all n variables and model n has 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4726a0",
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "plot_stats"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_stats <- function(model_stats_df) {\n",
    "  \n",
    "  pos_mods <- nrow(model_stats_df)\n",
    "  \n",
    "  if(pos_mods <25){by=1} else{by=5}\n",
    "  \n",
    "  model_stats_df %>%\n",
    "    dplyr::select(model, \n",
    "                  mean_kge, mean_nse, mean_mae, mean_pbias,\n",
    "                  sd_kge,  sd_nse,  sd_mae, sd_pbias) %>%\n",
    "    pivot_longer(cols = -model, names_to = c(\".value\", \"metric\"), names_sep = \"_\") %>%\n",
    "    ggplot() +\n",
    "      geom_line(aes(x = model, y = mean, \n",
    "                    color = metric)\n",
    "                ) +\n",
    "      geom_errorbar(aes(x = model, \n",
    "                        ymin = mean-sd, ymax = mean + sd,\n",
    "                        color = metric),\n",
    "                    alpha = 0.4\n",
    "                    ) +\n",
    "      geom_point(aes(x = model, y = mean, color = metric),\n",
    "                shape = 19, size = 1) +\n",
    "      scale_color_brewer(palette = \"Set1\",\n",
    "                         guide = \"none\") + \n",
    "      scale_x_continuous(breaks = seq(0,pos_mods,by),\n",
    "                         minor_breaks = seq(0,pos_mods,1)) + \n",
    "      labs(y = element_blank(),\n",
    "           x = \"Model Iteration\") + \n",
    "      theme_few() +\n",
    "      theme(panel.grid = element_line(color = \"gray90\"),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.placement = \"outside\") +\n",
    "      facet_wrap(~metric, scales = \"free\", ncol = 1,\n",
    "                 strip.position = \"left\")\n",
    "\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1555ee",
   "metadata": {},
   "source": [
    "### The runner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37145e7",
   "metadata": {
    "lines_to_next_cell": 0,
    "name": "lgbm_runner"
   },
   "outputs": [],
   "source": [
    "\n",
    "lgbm_runner <- function(train_df = NULL, \n",
    "                        test_df = NULL,\n",
    "                        chosen_mod = NULL,\n",
    "                        is_tuned = FALSE,\n",
    "                        tuned_params = NULL,\n",
    "                        conc_scalars = NULL){\n",
    "  \n",
    "  \n",
    "    ########## Model setup #########################################################\n",
    "    \n",
    "    ### Declare the predictor and response variables \n",
    "    ### Make sure to exclude variables we left in there \n",
    "    ### For interpretability\n",
    "    \n",
    "    preds <- data.matrix(train_df %>%\n",
    "                           dplyr::select(chosen_mod$Feature))\n",
    "                                                                        \n",
    "                            \n",
    "    response <- train_df$log_conc\n",
    "                            \n",
    "    ### Set up the environment - this is just preparing the dataset API for use by lightgbm.\n",
    "    #### This is our training data\n",
    "    train_lgbm <- lgb.Dataset(preds, label = response)\n",
    "                            \n",
    "    #### Declare the test data\n",
    "    test_lgbm <- data.matrix(test_df %>%\n",
    "                           dplyr::select(chosen_mod$Feature))\n",
    "                        \n",
    "    ### Declare the hyperparameters \n",
    "                    \n",
    "    if(is_tuned == FALSE ) { \n",
    "      \n",
    "      hyperparams <- list(objective = \"regression\",\n",
    "                          num_leaves = 31L,\n",
    "                          learning_rate = 0.1,\n",
    "                          min_data_in_leaf = 20L,\n",
    "                          num_threads = 10L)\n",
    "      } else if(is_tuned == TRUE ) {\n",
    "        \n",
    "        hyperparams <- list(objective = \"regression\",\n",
    "                            num_leaves = tuned_params$num_leaves,\n",
    "                            min_data_in_leaf = tuned_params$min_n,\n",
    "                            bagging_fraction = tuned_params$sample_size,\n",
    "                            bagging_freq = 1,\n",
    "                            num_iterations = tuned_params$trees,\n",
    "                            max_depth = tuned_params$tree_depth\n",
    "                            )\n",
    "                        }\n",
    "                        \n",
    "\n",
    "    \n",
    "    ################################################################################\n",
    "    ################################################################################\n",
    "    \n",
    "    ### Now, let's do some actual modeling stuff\n",
    "    \n",
    "    ### Inform what we are running:\n",
    "    \n",
    "    cat(crayon::cyan(\"Running model with Features:\"),\n",
    "        crayon::green(chosen_mod$Feature),\n",
    "        crayon::green(\"\\n\"))\n",
    "\n",
    "    \n",
    "    #### Train the model\n",
    "                            \n",
    "    set.seed(913)\n",
    "                            \n",
    "    model_lgbm <- lgb.train(hyperparams,\n",
    "                                   data = train_lgbm,\n",
    "                                   verbose = 1L,\n",
    "                                   nrounds = 100L)\n",
    "                            \n",
    "    ### Predict with the model on test data\n",
    "    \n",
    "    predicted <- predict(model_lgbm, \n",
    "                                      data = test_lgbm) %>%\n",
    "      as_tibble() %>% \n",
    "      rename(log_predicted_conc = 1)\n",
    "                            \n",
    "                            \n",
    "    ### Bind predictions on test data to observations\n",
    "    \n",
    "    pred_obs <- bind_cols(test_df %>% \n",
    "                                      dplyr::rename(log_observed_conc = log_conc),\n",
    "                                    predicted) \n",
    "    \n",
    "    ### Now use the scalars to convert back to linear scale\n",
    "    \n",
    "    rescale_pred_obs <- pred_obs %>%\n",
    "      mutate(observed_conc = 10^(log_observed_conc*conc_scalars$sd + conc_scalars$mean),\n",
    "             predicted_conc = 10^(log_predicted_conc*conc_scalars$sd + conc_scalars_test$mean))\n",
    "    \n",
    "                        \n",
    "    ### Evaluate - we are going to use multiple error metrics\n",
    "    \n",
    "    #### For each watershed\n",
    "    \n",
    "    model_stats_each <- rescale_pred_obs %>%\n",
    "      ungroup() %>%\n",
    "      dplyr::group_by(tributary) %>%\n",
    "      summarise(mae = hydroGOF::mae(predicted_conc, observed_conc),\n",
    "                nse = hydroGOF::NSE(predicted_conc, observed_conc),\n",
    "                kge = hydroGOF::KGE(predicted_conc, \n",
    "                                    observed_conc),\n",
    "                pbias = hydroGOF::pbias(predicted_conc,\n",
    "                                        observed_conc)) %>%\n",
    "      dplyr::ungroup() \n",
    "    \n",
    "    #### Median across all \n",
    "    \n",
    "    model_stats_summary <- model_stats_each %>%\n",
    "      reframe(across(where(is.numeric),\n",
    "                     list(median = ~median(.x)),\n",
    "                     .names = \"{.fn}_{.col}\"\n",
    "                     ))\n",
    "    \n",
    "    \n",
    "    ##### Return stuff\n",
    "    \n",
    "    test_stats <- list()\n",
    "    \n",
    "    test_stats[[1]] <- model_stats_each\n",
    "    \n",
    "    test_stats[[2]] <- model_stats_summary\n",
    "    \n",
    "    test_stats[[3]] <- rescale_pred_obs %>%\n",
    "      dplyr::select(tributary, date, observed_conc, predicted_conc)\n",
    "    \n",
    "    return(test_stats)\n",
    "\n",
    "  \n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643ea7e",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,name,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
