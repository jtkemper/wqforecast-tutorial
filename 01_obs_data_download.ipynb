{
 "cells": [
  {
   "cell_type": "raw",
   "id": "75fe4962",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"01_obs_data_download\"\n",
    "author: \"JTK\"\n",
    "date: \"2025-05-01\"\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf32628",
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65e2ee",
   "metadata": {},
   "source": [
    "################################################################################\n",
    "\n",
    "This script downloads flow data from the pre-selected gages of interest. \n",
    "It also discovers stations with water quality data gathered by Vermont DEC \n",
    "corresponding to each of the streamflow gages and downloads that data. \n",
    "We have specifically tailored this approach to the Lake Champlain basin, but it is\n",
    "flexible enough so that interested users can modify various lines to download\n",
    "from the gages and sampling sites of interest to them. Most of this code will take\n",
    "too long to run in a workshop capacity, but we have decided to include it here for \n",
    "completeness' sake. For the purposes of this workshop, we will simply read-in .csv files\n",
    "with the requisite data\n",
    "\n",
    "**Required inputs**\n",
    "\n",
    "1) NWIS site numbers for sites of interest (data/lake_champlain_usgs_gages.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7431d79",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Outputs/Returns**\n",
    "\n",
    "1) Raw dataframe of daily streamflow data for 18 watersheds in Lake Champlain\n",
    "\n",
    "2) Raw water quality dataframe for total phosphorus and chloride concentration \n",
    "as measured in each of those 18 watersheds\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1be3a",
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5c34b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ee7e6",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "## Data mgmt\n",
    "require(tidyverse)\n",
    "\n",
    "## Data download\n",
    "require(dataRetrieval)\n",
    "require(nhdplusTools)\n",
    "require(EPATADA)\n",
    "\n",
    "## Misc.\n",
    "require(here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08244931",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Get metadata for USGS gage of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963669eb",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### First, retrieve the NWIS IDs of the gages we have selected\n",
    "#### in the Lake Champlain basin\n",
    "#### We've pulled these gage IDs from various pubs in the region \n",
    "#### (Underwood et al., 2017, WRR; Vaughn, 2019 LCBP Report)\n",
    "\n",
    "lc_site_ids <- read_csv(here(\"input-data/lake_champlain_usgs_gages.csv\")) %>%\n",
    "  dplyr::select(river_basin, site_no)\n",
    "\n",
    "#### Make sure the NWIS code is eight digits\n",
    "#### .csv files often remove the leading zeros from ids\n",
    "\n",
    "lc_site_ids <- lc_site_ids %>%\n",
    "  mutate(site_no = as.character(sprintf(\"%08d\", site_no)))\n",
    "\n",
    "#### Get official site names and various other pieces of metadata\n",
    "\n",
    "##### Specific to gage location and flow monitoring record\n",
    "\n",
    "lc_gages_metadata <- whatNWISdata(siteNumber = lc_site_ids$site_no,\n",
    "             parameterCd = \"00060\",\n",
    "             service = \"dv\") \n",
    "\n",
    "##### Drainage area\n",
    "\n",
    "lc_gages_metadata_da <- readNWISsite(lc_site_ids$site_no) %>%\n",
    "  dplyr::select(site_no, drain_area_va)\n",
    "\n",
    "##### And NHDPlus (Medium-res) COMID\n",
    "##### Which is absolutely essential for working with the NWM\n",
    "##### To do this we must first get the station IDs into the format needed\n",
    "##### To query the NHD for the COMID related to each site location\n",
    "\n",
    "lc_gages_nldi <- lc_site_ids %>%\n",
    "  mutate(fsrc = \"nwissite\",\n",
    "         fid = paste0(\"USGS-\", site_no)) %>%\n",
    "  mutate(comid = map2_dbl(fsrc, fid,\n",
    "                      ~discover_nhdplus_id(nldi_feature = list(featureSource = .x, \n",
    "                                                featureID = .y)))) %>%\n",
    "  dplyr::select(!c(\"river_basin\", \"fsrc\"))\n",
    "\n",
    "#### And clean up the metadata\n",
    "\n",
    "lc_gages_metadata_clean <- inner_join(lc_site_ids, \n",
    "                                      lc_gages_nldi,\n",
    "                                      by = \"site_no\") %>%\n",
    "  inner_join(.,   lc_gages_metadata %>%\n",
    "               dplyr::select(station_nm, site_no, \n",
    "                             dec_lat_va, dec_long_va, dec_coord_datum_cd,\n",
    "                             begin_date, end_date),\n",
    "             by = \"site_no\") %>%\n",
    "  inner_join(., lc_gages_metadata_da,\n",
    "             by = \"site_no\") %>%\n",
    "  rename(drain_area_mi2 = drain_area_va) %>%\n",
    "  mutate(drain_area_km2 = drain_area_mi2*2.58999) %>%\n",
    "  dplyr::select(!drain_area_mi2) %>%\n",
    "  relocate(drain_area_km2, .after = \"station_nm\") %>%\n",
    "  rename(tributary = river_basin) %>%\n",
    "  relocate(station_nm, .after = \"tributary\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccf100",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Get metadata for water quality monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dccd6f",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Now, read-in a list of the VTDEC tributary monitoring sites. \n",
    "#### To do so, we have to download data monitored by VTDEC \n",
    "#### for a subset of the Champlain Tribs water quality monitoring period (1991-present). \n",
    "#### Let's take a slice of 2012\n",
    "#### Otherwise it would take waaaaaay too long to download\n",
    "#### 2012, according to VTDEC's webpage, should be a period when monitoring in all tribs\n",
    "#### is active\n",
    "\n",
    "#### Note that this returns every sample sampled by VTDEC in this slice of 2012\n",
    "\n",
    "wq_data_profile <- EPATADA::TADA_DataRetrieval(\n",
    "                           organization = \"1VTDECWQ\",\n",
    "                           startDate = \"2012-03-01\",\n",
    "                           endDate = \"2012-09-30\",\n",
    "                           applyautoclean = FALSE)\n",
    "\n",
    "#### Map locations of samples and inspect to make sure they fall in both\n",
    "#### VT & NY and include all the tribs we want\n",
    "\n",
    "EPATADA::TADA_OverviewMap(wq_data_profile %>%\n",
    "                         rename(TADA.LatitudeMeasure = ActivityLocation.LatitudeMeasure,\n",
    "                                TADA.LongitudeMeasure = ActivityLocation.LongitudeMeasure,\n",
    "                                TADA.CharacteristicName = CharacteristicName) %>%\n",
    "                         mutate(TADA.LatitudeMeasure = as.numeric(TADA.LatitudeMeasure),\n",
    "                                TADA.LongitudeMeasure =as.numeric(TADA.LongitudeMeasure)))\n",
    "\n",
    "#### Select river monitoring sites that fall along the main stem of each tributary\n",
    "#### Then, slice by the number of samples in our sample monitoring period\n",
    "#### We will select the site with the most samples, which we know will correspond\n",
    "#### to the main monitoring site for water quality for each of the tribs\n",
    "#### (which is the site we want)\n",
    "#### We will then use these site ids to download all the water quality data for the monitoring period\n",
    "\n",
    "site_ids_wq <- wq_data_profile %>%\n",
    "  filter(MonitoringLocationName %in% lc_gages_metadata_clean$tributary) %>%\n",
    "  dplyr::select(MonitoringLocationIdentifier, MonitoringLocationName, ResultIdentifier) %>%\n",
    "  dplyr::group_by(MonitoringLocationIdentifier,MonitoringLocationName) %>%\n",
    "  summarise(sample_count = length(unique(ResultIdentifier))) %>%\n",
    "  dplyr::ungroup() %>%\n",
    "  dplyr::group_by(MonitoringLocationName) %>%\n",
    "  slice_max(sample_count)\n",
    "\n",
    "#### Now plot again to make sure that we selected the right ones\n",
    "\n",
    "EPATADA::TADA_OverviewMap(wq_data_profile %>%\n",
    "                         rename(TADA.LatitudeMeasure = ActivityLocation.LatitudeMeasure,\n",
    "                                TADA.LongitudeMeasure = ActivityLocation.LongitudeMeasure,\n",
    "                                TADA.CharacteristicName = CharacteristicName) %>%\n",
    "                         mutate(TADA.LatitudeMeasure = as.numeric(TADA.LatitudeMeasure),\n",
    "                                TADA.LongitudeMeasure =as.numeric(TADA.LongitudeMeasure)) %>%\n",
    "                         filter(MonitoringLocationIdentifier %in% site_ids_wq$MonitoringLocationIdentifier))\n",
    "\n",
    "#### And finally join the stream gage metadata file so that we have all\n",
    "#### station identifiers in one location\n",
    "\n",
    "lc_sites_metadata_all <- inner_join(lc_gages_metadata_clean,\n",
    "           site_ids_wq %>%\n",
    "             dplyr::select(MonitoringLocationIdentifier, MonitoringLocationName) %>%\n",
    "             rename(wq_site_id = MonitoringLocationIdentifier,\n",
    "                    tributary = MonitoringLocationName),\n",
    "           by = \"tributary\") %>%\n",
    "  relocate(wq_site_id, .after = fid)\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "#### Remove extraneous variables\n",
    "\n",
    "rm(wq_data_profile)\n",
    "\n",
    "################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baa488",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Download Data\n",
    "\n",
    "### Streamflow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd2477",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Downloads USGS flow data \n",
    "#### We want to daily values\n",
    "#### so let's download them\n",
    "\n",
    "flow_data <- dataRetrieval::readNWISdv(siteNumbers = \"04292810\", \n",
    "                          parameterCd = \"00060\",\n",
    "                          startDate = \"1990-01-01\",\n",
    "                          endDate = \"2023-12-31\") %>%\n",
    "  renameNWISColumns() %>%\n",
    "  addWaterYear() %>%\n",
    "  dplyr::filter(!str_detect(Flow_cd , \"P\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439a1e7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Water quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c285333",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Download water quality data from each of the eighteen tribs\n",
    "#### This require looping over each station to avoid breaking the downloader\n",
    "\n",
    "tribs_wq <- list()\n",
    "\n",
    "for(i in 1:length(lc_sites_metadata_all$wq_site_id)) {\n",
    "  \n",
    "  cat(crayon::cyan(\"Reading\", lc_sites_metadata_all$tributary[i], \"\\n\"))\n",
    "  \n",
    "  wq_by_site <- TADA_DataRetrieval(siteid = lc_sites_metadata_all$wq_site_id[i],\n",
    "                           startDate = \"1990-01-01\",\n",
    "                           endDate = \"2023-12-31\",\n",
    "                           applyautoclean = FALSE)\n",
    "  \n",
    "  \n",
    "  tribs_wq[[i]] <- wq_by_site\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "lc_tribs_wq_all <- bind_rows(tribs_wq)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d396b6c",
   "metadata": {},
   "source": [
    "# Retrieve all data from files\n",
    "\n",
    "####### The above code is just for illustration purposes. Here, we are just pulling in the files that we have pre-downloaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed19f5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34d73a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "name,eval,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
