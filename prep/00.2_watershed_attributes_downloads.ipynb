{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b6ed7b6e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"00.2_watershed_attributes_downloads\"\n",
    "author: \"JTK\"\n",
    "date: \"2025-05-02\"\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3e987",
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "setup",
    "tags": [
     "remove_cell",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d67aef",
   "metadata": {},
   "source": [
    "################################################################################\n",
    "\n",
    "This script downloads various static watershed attributes for each individual watershed\n",
    "in the Lake Champlain Basin. It draws from a variety of sources, including the\n",
    "National Hydrography Dataset (high- and medium-res), US (SSURGO) and Canadian\n",
    "soils datasets, USGS StreamStats, a USGS-built set of expanded for the NHD \n",
    "(Wieczorek et al., 2018, https://doi.org/10.5066/F7765D7V.), and several other \n",
    "publically available datasets. \n",
    "\n",
    "We then compile these into one large dataframe that contains watershed attributes\n",
    "for each basin. The goal in doing this is to develop \"global\" machine learning models\n",
    "that may potentially learn relationships between dynamic hydrology and static watershed\n",
    "attributes, allowing them to learn from a diversity of data (i.e., observations in all watersheds)\n",
    "to make predictions in individual watersheds\n",
    "\n",
    "As with the previous script, we are mainly including these here for illustrative purposes\n",
    "and so that you may leave the workshop with a set of an example codes that can be \n",
    "run together to demonstrate the workflow from start to finish. To keep on time for the workshop, \n",
    "we will include the output of this script as an input .csv\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "1) Shapefile that contains outlet points for each watershed\n",
    "\n",
    "2) .csv of land cover data derived from Troy et al., 2007\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "1) Wide dataframe of static watershed attributes for each of the 18 Lake Champlain\n",
    "tributaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa248d1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "################################################################################\n",
    "\n",
    "# Housekeeping\n",
    "\n",
    "### Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90969e",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Data mgmt \n",
    "require(tidyverse)\n",
    "require(tsibble)\n",
    "\n",
    "### Hydrography & watershed characteristics\n",
    "require(streamstats)\n",
    "require(nhdplusTools)\n",
    "require(dataRetrieval)\n",
    "require(sbtools)\n",
    "\n",
    "### Spatial \n",
    "require(sf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfbf14",
   "metadata": {},
   "source": [
    "# Get location data\n",
    "\n",
    "### Import watershed outlet points\n",
    "\n",
    "These points were delienated manually in ArcGIS by selecting the flowline closest\n",
    "to the watershed outlet and transforming it into a point feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05ad1f",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Import 'em\n",
    "\n",
    "start_points <- st_read(here(\"input-data/watershed_outlets/watershed_outlets_updated.shp\")) %>%\n",
    "  st_zm(drop = TRUE) %>%\n",
    "  filter(tributary != \"Putnam Creek\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4538a385",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Find COMID (for NHD Medium-Res) for outlet points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b54f1",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Convert the simple feature collection to a set of point features \n",
    "\n",
    "start_points2 <- start_points[\"geometry\"] %>% sf::st_as_sfc()\n",
    "\n",
    "#### Create a list to save the medium-res comids\n",
    "\n",
    "comid_start <- list()\n",
    "\n",
    "#### Find COMIDs corresponding to outlet points\n",
    "\n",
    "for(i in 1:length(start_points2)) {\n",
    "  \n",
    "  print(i)\n",
    "  \n",
    "   start <- tibble(start = discover_nhdplus_id(start_points2[i]))\n",
    "    \n",
    "  \n",
    "  comid_start[i] <- start\n",
    "  \n",
    "}\n",
    "\n",
    "#### Bind them all together into one dataframe\n",
    "\n",
    "start_comid_df <- tibble(start_comid = unlist(comid_start)) %>%\n",
    "  bind_cols(start_points, .) %>%\n",
    "  as_tibble() %>%\n",
    "  dplyr::select(!geometry) %>%\n",
    "  rename(comid = start_comid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34612c6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Get all NHD Medium-Res comids in a particular basin\n",
    "\n",
    "This will be necessary if we want to use available USGS datasets to determine\n",
    "various attributes at each flowline in the entire basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dccfb",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Get all COMIDs in an individual basin\n",
    "\n",
    "comids_by_basin <- map2(start_comid_df$tributary,\n",
    "                        start_comid_df$start_comid,\n",
    "                        all_comid_getter)\n",
    "\n",
    "#### Bind them all together\n",
    "\n",
    "comids_by_basin <- comids_by_basin %>%\n",
    "  bind_rows() %>%\n",
    "  unnest(data) %>%\n",
    "  rename(comid = nhdplus_comid) %>%\n",
    "  mutate(comid = as.integer(comid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e928a1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Find outlet COMIDs for NHD high-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9046f2",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "#### First, download the NHD high-resolution\n",
    "\n",
    "lc_nhd_hr <- download_nhdplushr(here(\"downloads/nhd_hr\"),\n",
    "                                hu_list = \"0430\")\n",
    "\n",
    "#### Get the name of the geodatabase\n",
    "\n",
    "lc_nhd_hr_gdb <- list.files(lc_nhd_hr, pattern = \".gdb\")\n",
    "\n",
    "#### And create a geopackage to save the data \n",
    "\n",
    "lc_hr_gpkg <- here(\"downloads/nhd_hr/0430_hr.gpkg\")\n",
    "\n",
    "#### Extract the NHD data from the downloaded geodatabase\n",
    "\n",
    "lc_hr <- get_nhdplushr(lc_nhd_hr,\n",
    "                       out_gpkg = lc_hr_gpkg)\n",
    "\n",
    "\n",
    "#### Find high-resolution COMIDs\n",
    "\n",
    "comid_start_hr <- list()\n",
    "\n",
    "for(i in 1:length(start_points2)) {\n",
    "  \n",
    "  print(i)\n",
    "  \n",
    "   start <- get_flowline_index(lc_hr$NHDFlowline,\n",
    "                   start_points2[i])\n",
    "    \n",
    "  \n",
    "  comid_start_hr[i] <- tibble(COMID = start$COMID)\n",
    "  \n",
    "}\n",
    "\n",
    "#### Bind them all together into one dataframe\n",
    "\n",
    "start_comid_hr <- tibble(start_comid = unlist(comid_start_hr)) %>%\n",
    "  bind_cols(start_points, .) %>%\n",
    "  as_tibble() %>%\n",
    "  dplyr::select(!geometry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a7b85",
   "metadata": {},
   "source": [
    "# Get StreamStats\n",
    "\n",
    "We are downloading the StreamStats parameters available for VT and NY. In particular,\n",
    "what we are interested in is percent elevation over 1200 ft, which for the LCB is a rough indicator \n",
    "of snow coverage/importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81834b70",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Take the shapefile of catchment outlet points \n",
    "#### And transform it to the format needed to access the StreamStats API\n",
    "#### (which requires an individual field for lat and for long)\n",
    "start_points_df <- start_points %>%\n",
    "  as_tibble() %>%\n",
    "  mutate(lon = st_coordinates(geometry)[,1],\n",
    "         lat = st_coordinates(geometry)[,2])\n",
    "\n",
    "#### Create an empty list to save returned data\n",
    "\n",
    "streamstat_features <- list()\n",
    "\n",
    "#### A loop to extract the workspace IDs for each watershed based on \n",
    "#### the outlet points we have imported\n",
    "#### The workspace IDs, I believe, are the internal StreamStats reference\n",
    "#### to the watershed that is delineated from any given point\n",
    "#### Once we have these, we can acquire the watershed features that we want\n",
    "\n",
    "for(i in 1:length(start_points_df$tributary)) {\n",
    "  \n",
    "  #### Track our progress across watersheds\n",
    "  \n",
    "  cat(crayon::cyan(\"\\nRetrieving\", start_points_df$tributary[i], \"\\n\"))\n",
    "  \n",
    "  #### Delineate the watershed in StreamStats and return watershed characteristics\n",
    "  #### but don't return the actual watershed polygon\n",
    "  \n",
    "  #### Do this in a while loop to repeat download if we haven't actually retrieved \n",
    "  #### watershed characteristic values\n",
    "  #### For some reason, sometimes StreamStats returns watershed \n",
    "  #### chars without associated values\n",
    "  #### When this happens, there are less than six columns in the returned\n",
    "  #### \"parameters\" dataframe\n",
    "  #### This while loop simply checks to see how many columns are in the \n",
    "  #### parameters dataframe and, if it's less than six (so no values),\n",
    "  #### repeats the download of watershed characteristics\n",
    "  \n",
    "  n_params <- 5\n",
    "  \n",
    "  while(n_params < 6){\n",
    "    \n",
    "    #### Do the download\n",
    "    \n",
    "    watershed <- delineateWatershed(start_points_df$lon[i],\n",
    "                                 start_points_df$lat[i],\n",
    "                                 crs = 4269,\n",
    "                                 includeparameters = \"true\",\n",
    "                                 includefeatures = \"false\")\n",
    "  \n",
    "    n_params <- ncol(watershed$parameters)\n",
    "    \n",
    "    #### Print to output if we need to repeat \n",
    "    \n",
    "    if(n_params<6){cat(crayon::yellow(\"\\nRepeating\\n\"))}\n",
    "   \n",
    "  } ## End while loop\n",
    "  \n",
    "\n",
    "  #### Nest the parameters and append which watershed they are for\n",
    "  \n",
    "  streamstat_features[[i]] <- watershed$parameters %>%\n",
    "    nest() %>%\n",
    "    mutate(tributary = start_points_df$tributary[i])\n",
    "  \n",
    "\n",
    "\n",
    "} ## End for loop\n",
    "\n",
    "#### Bind all watersheds together \n",
    "\n",
    "all_streamstat_features <- bind_rows(streamstat_features)\n",
    "\n",
    "#### Now trim the features down to just the two that we want\n",
    "\n",
    "final_streamstat_features <- all_streamstat_features %>%\n",
    "  unnest(cols = c(data)) %>%\n",
    "  filter(str_detect(name, \"1200 ft|Drainage\")) %>%\n",
    "  dplyr::select(tributary, code, value) %>%\n",
    "  mutate(code = tolower(code)) %>%\n",
    "  pivot_wider(names_from = code, values_from = value) %>%\n",
    "  mutate(drnarea_km2 = drnarea*2.58999) %>% ### transform from mi^2 to km^2\n",
    "  dplyr::select(tributary, drnarea_km2, el1200)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf330853",
   "metadata": {},
   "source": [
    "# Get land use attributes\n",
    "\n",
    "We are importing percentage values for land use that we calculated in ArcGIS. The\n",
    "land use layer we used is one specific to the LCB that was created to model phosphorus\n",
    "contributions to the lake. You can read more about its creation in Troy et al., 2007 \n",
    "(http://www.lcbp.org/techreportPDF/54_LULC-Phosphorus_2007.pdf) and you can download\n",
    "the layer here \n",
    "(https://www.arcgis.com/home/item.html?id=16043a36e8a64aa79cb1728cf7d98409)\n",
    "\n",
    "We determined the percentage of each land cover class by simply totaling up the \n",
    "area of the pixels in each watershed and dividing by the watershed area\n",
    "\n",
    "Similarly, we created a 100 m buffer around all the flowlines in the basin and totalled\n",
    "up the area for each landcover class within this buffer, and then divided by total watershed\n",
    "area. This gives us an idea of riparian land cover. \n",
    "\n",
    "We import both these datasets here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6b89a",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Get land cover for each basin entirely\n",
    "\n",
    "lulc_2001 <- lulc_processer(here(\"data/lulc_2001_by_watershed.csv\"),\n",
    "                            final_streamstat_features) %>%\n",
    "  drop_na()\n",
    "\n",
    "#### Get land cover for riparian areas\n",
    "\n",
    "lulc_all_100m <- lulc_processer(here(\"data/lulc_stats_all_100m.csv\"),\n",
    "                            final_streamstat_features) %>%\n",
    "  drop_na() %>%\n",
    "  rename_if(is.numeric, ~paste0(\"all_100m_\", .))\n",
    "\n",
    "#### Join together\n",
    "\n",
    "lulc_all <- inner_join(lulc_2001, lulc_all_100m, by = \"tributary\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899cdb8",
   "metadata": {},
   "source": [
    "# Get some hydrography attributes\n",
    "\n",
    "Here, we want to get a few things from the NHD HR that may impact total phosphorus\n",
    "and chloride contributions. Namely, these are things like drainage density (which\n",
    "we derive from flowline lengths), stream slopes, lengths of different geomorphic\n",
    "stream types, and frequency of particular stream orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0db685",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Get some stats\n",
    "\n",
    "flowline_stats_hr <- map2(start_comid_hr$tributary,\n",
    "                                start_comid_hr$start_comid,\n",
    "                                .f = hr_flow_length_calculator,\n",
    "                                hr_data = lc_hr$NHDFlowline,\n",
    "                                hr_geopack = lc_hr_gpkg,\n",
    "                                \"Trib\",\n",
    "                                .progress = TRUE)\n",
    "\n",
    "#### Extract the total flowline length in each tributary\n",
    "\n",
    "flow_length_hr <- bind_cols(start_comid_hr,\n",
    "                            flowline_length_km = map_dbl(flowline_stats_hr, \n",
    "                                                         1))\n",
    "#### Get stream relief\n",
    "#### Elevation in NHD HR is in centimeters, so we must convert\n",
    "\n",
    "relief_m <-  bind_cols(start_comid_hr,\n",
    "                            max_elev = map_dbl(flowline_stats_hr, \n",
    "                                                         5)/100,\n",
    "                       min_elev = map_dbl(flowline_stats_hr,6)/100) %>%\n",
    "  mutate(relief = max_elev - min_elev) %>%\n",
    "  rename(stream_relief_m = relief) %>%\n",
    "  dplyr::select(tributary, stream_relief_m)\n",
    "\n",
    "#### Extract the percentage (by length) of each stream order \n",
    "\n",
    "pct_orders <- bind_cols(start_comid_hr, map_dfr(flowline_stats_hr, ~(.[[3]] %>%\n",
    "                               as_tibble() %>%\n",
    "                               nest()))) %>%\n",
    "  unnest(data) %>%\n",
    "  pivot_wider(names_from = StreamOrde, names_glue = \"pct_{StreamOrde}_order\",\n",
    "              values_from = pct_order) %>%\n",
    "  dplyr::select(!start_comid) %>%\n",
    "  replace(is.na(.), 0) \n",
    "\n",
    "#### Extract the percentage (by length) of each geomorphic stream type\n",
    "#### Channel type is based on slope thresholds and calculated based on those listed in \n",
    "#### Geomorphic Classification of Rivers: An Updated Review\n",
    "#### (https://www.fs.usda.gov/rm/pubs_journals/2022/rmrs_2022_buffington_j001.pdf))\n",
    "\n",
    "pct_by_type <- bind_cols(start_comid_hr, map_dfr(flowline_stats_hr, ~(.[[9]] %>%\n",
    "                               as_tibble() %>%\n",
    "                               nest()))) %>%\n",
    "  unnest(data) %>%\n",
    "  pivot_wider(names_from = channel_type, names_glue = \"{channel_type}_pct\",\n",
    "              values_from = pct_by_type) %>%\n",
    "  dplyr::select(!start_comid) %>%\n",
    "  replace(is.na(.), 0) \n",
    "\n",
    "\n",
    "#### And calculate drainage density based on the total flowline length\n",
    "#### And the basin area\n",
    "\n",
    "basin_drainage_density <- full_join(flow_length_hr %>%\n",
    "                                      dplyr::select(!start_comid),\n",
    "                                    final_streamstat_features %>%\n",
    "                                      dplyr::select(tributary, drnarea_km2),\n",
    "                                    by = \"tributary\") %>%\n",
    "  mutate(drain_density_km_km2 = flowline_length_km/drnarea_km2) %>%\n",
    "  dplyr::select(tributary, drain_density_km_km2)\n",
    "\n",
    "\n",
    "\n",
    "#### And now get the length of the mainstem for each tributary\n",
    "\n",
    "##### Calculate it\n",
    "\n",
    "mainstem_flowline_lengths <- map2(start_comid_hr$tributary,\n",
    "                                start_comid_hr$start_comid,\n",
    "                                hr_flow_length_calculator,\n",
    "                                hr_data = lc_hr$NHDFlowline,\n",
    "                                hr_geopack = lc_hr_gpkg,\n",
    "                                \"Main\",\n",
    "                                .progress = TRUE)\n",
    "\n",
    "##### And bind together\n",
    "\n",
    "mainstem_flowline_lengths <- bind_cols(start_comid_hr %>%\n",
    "                                          dplyr::select(tributary),\n",
    "                                       mainstem_length_km = map_dbl(\n",
    "                                         mainstem_flowline_lengths, 1)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14dc99a",
   "metadata": {},
   "source": [
    "# Manually calculate some other watershed characteristics\n",
    "\n",
    "### Hack's exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129ac51",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "hacks <- inner_join(final_streamstat_features %>%\n",
    "             dplyr::select(tributary, drnarea_km2),\n",
    "           mainstem_flowline_lengths %>%\n",
    "             dplyr::select(tributary, mainstem_length_km), \n",
    "           by = \"tributary\") %>%\n",
    "  mutate(across(where(is.numeric), ~log10(.))) %>%\n",
    "  rename_with(~paste0(\"log_\", .), where(is.numeric)) %>%\n",
    "  mutate(h = log_mainstem_length_km/log_drnarea_km2) %>%\n",
    "  dplyr::select(tributary, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85c2ef",
   "metadata": {},
   "source": [
    "### Richard-Baker Flashiness Index\n",
    "\n",
    "Calculates the Richards-Baker Flashiness Index, an indicator of hydrologic \n",
    "\"flashiness\". More details can be found in the original publication\n",
    "(https://doi.org/10.1111/j.1752-1688.2004.tb01046.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f492c",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Make the flow data into a tsibble to check to see if there are \n",
    "#### gaps in the flow data\n",
    "\n",
    "flow_ts <- flow_data %>%\n",
    "  as_tibble() %>%\n",
    "  mutate(dateTime = as_date(dateTime)) %>%\n",
    "    as_tsibble(key = site_no, index = dateTime)\n",
    "\n",
    "\n",
    "\n",
    "#### Check for gaps\n",
    "\n",
    "time_gaps <- has_gaps(flow_ts) %>%\n",
    "  filter(.gaps == TRUE)\n",
    "\n",
    "\n",
    "#### Extract the sites with gaps\n",
    "\n",
    "sites_with_gaps <- flow_ts %>%\n",
    "  filter(site_no %in% time_gaps$site_no)\n",
    "\n",
    "#### Find where the gaps are\n",
    "\n",
    "gaps <- count_gaps(flow_ts)\n",
    "\n",
    "#### If gaps are small\n",
    "#### We can fill the gaps with linear interpolation\n",
    "\n",
    "flow_filled_gaps <- flow_ts %>%\n",
    "  fill_gaps()\n",
    "\n",
    "#### However, if they are large, we can just trim the flow data\n",
    "#### to only a period without gaps\n",
    "#### And assume the flashiness has not changed too drastically outside that period\n",
    "flow_ts <- flow_ts %>%\n",
    "  filter(waterYear < 2015)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Check to see that the number of gaps we filled with NAs\n",
    "#### is equal to the number of gaps there are\n",
    "\n",
    "identical(nrow(flow_filled_gaps %>%\n",
    "                   filter(is.na(Flow))),\n",
    "            sum(gaps$.n))\n",
    "\n",
    "\n",
    "#### Calculate the Richards-Baker Flashiness Index\n",
    "#### From https://doi.org/10.1111/j.1752-1688.2004.tb01046.x\n",
    "\n",
    "flashiness <- flow_ts %>%\n",
    "  as_tibble() %>%\n",
    "  mutate(Flow = Flow*0.0283168) %>%\n",
    "  dplyr::group_by(site_no) %>%\n",
    "  arrange(dateTime, .by_group = TRUE) %>%\n",
    "  mutate(abs_delta_daily_q = abs(Flow - lag(Flow))) %>%\n",
    "  drop_na() %>%\n",
    "  summarise(path_length = sum(abs_delta_daily_q, na.rm = TRUE),\n",
    "            total_q = sum(Flow, na.rm = TRUE)) %>%\n",
    "  mutate(rb_flashiness = path_length/total_q) %>%\n",
    "  inner_join(., lc_sites_metadata_all %>%\n",
    "               dplyr::select(tributary, site_no),\n",
    "             by = \"site_no\") %>%\n",
    "  dplyr::select(tributary, \n",
    "                rb_flashiness\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b63fd",
   "metadata": {},
   "source": [
    "### Flow anomaly\n",
    "\n",
    "This is from Underwood et al., 2018 ( https://doi.org/10.1002/2017WR021353), and is \n",
    "representative of the degree to which flow fluctuates at an annual scale. \n",
    "It is the ratio of mean annual peak flow to mean annual mean flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0715b",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "#### First calcualte the average annual daily flow\n",
    "\n",
    "mean_flows <- flow_data %>%\n",
    "  dplyr::group_by(site_no, waterYear) %>%\n",
    "  summarise(mean_annual_flow = mean(Flow)) %>%\n",
    "  dplyr::ungroup() %>%\n",
    "  dplyr::group_by(site_no) %>%\n",
    "  summarise(mean_annual_mean = mean(mean_annual_flow)) %>%\n",
    "  arrange(site_no) %>%\n",
    "  dplyr::ungroup()\n",
    "\n",
    "#### Then, get all peak flows for each station\n",
    "\n",
    "annual_peaks <- dataRetrieval::readNWISpeak(lc_sites_metadata_all$site_no, \n",
    "                            startDate = \"1990-10-01\",\n",
    "                            endDate = \"2022-09-30\") \n",
    "\n",
    "mean_annual_peaks <- annual_peaks %>%\n",
    "  dplyr::group_by(site_no) %>%\n",
    "  summarise(mean_annual_peak_flow = mean(peak_va, na.rm = TRUE)) %>%\n",
    "  dplyr::ungroup()\n",
    "\n",
    "#### Then calculate the flow anomaly by joining the two dataframes\n",
    "#### And taking the ratio of mean annual peak to mean annual mean flow\n",
    "\n",
    "flow_anomaly <- full_join(mean_flows, mean_annual_peaks, by = \"site_no\") %>%\n",
    "  inner_join(., lc_sites_metadata_all %>%\n",
    "               dplyr::select(tributary, site_no),\n",
    "             by = \"site_no\") %>%\n",
    "  dplyr::mutate(peak_flow_anom = mean_annual_peak_flow/mean_annual_mean) %>%\n",
    " dplyr::select(tributary, peak_flow_anom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b9b44",
   "metadata": {},
   "source": [
    "# Get tile drainage\n",
    "\n",
    "Here, we utilize a dataset constructed by Valayamkunnath et al., 2020 \n",
    "(https://doi.org/10.1038/s41597-020-00596-x), the metadata for which can be \n",
    "found at https://doi.org/10.6084/m9.figshare.12668234 and the data for which can\n",
    "be found here: https://figshare.com/articles/dataset/AgTile-US/11825742. This a layer \n",
    "that maps subsurface tile drainage at 30-m resolution across CONUS. \n",
    "A large body of prior research has shown that the extent of tile drainage within\n",
    "a watershed has a notable impact on hydrological, nutrient, and various other \n",
    "constituent dynamics. We have trimmed this data to only our watersheds of interest\n",
    "in ArcGIS. We import that trimmed data set here, and then calculate tile drainage\n",
    "as a percent of watershed area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b90e4",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Read in the .csv file \n",
    "tile <- read_csv(here(\"data/percent_tile.csv\")) %>%\n",
    "  as_tibble() %>%\n",
    "  rename_all(~tolower(.)) %>%\n",
    "  rename(area_m2 = area) %>%\n",
    "  mutate(area_km2 = area_m2 * 1E-6)\n",
    "\n",
    "#### Join to StreamStats data to get watershed area\n",
    "#### And calculate tile drainage as a percentage of watershed area\n",
    "\n",
    "tile_percent_by_basin <- tile %>%\n",
    "  mutate(value = ifelse(value == 1, \"tile\", \"no_tile\")) %>%\n",
    "  dplyr::group_by(tributary) %>%\n",
    "  filter(value == \"tile\") %>%\n",
    "  dplyr::select(tributary, area_km2) %>%\n",
    "  rename(area_tile_drain_km2 = area_km2) %>%\n",
    "  inner_join(., final_streamstat_features,\n",
    "             by = \"tributary\") %>%\n",
    "  mutate(pct_drained_by_tile = area_tile_drain_km2/drnarea_km2*100) %>%\n",
    "  dplyr::select(tributary, pct_drained_by_tile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebe437",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Get various other watershed attributes\n",
    "\n",
    "For the rest of these, we will be relying on Wieczorek et al., 2018 (https://doi.org/10.5066/F7765D7V.), \n",
    "a USGS dataset that relates various watershed attributues to each COMID within \n",
    "the NHD medium resolution. This dataset has various values for each attributue of interest:\n",
    "cat_xxx and tot_xxx, which, respective, are the value of the attribute of interest AT \n",
    "the flowline (COMID) of interest and the average value of the attribute of interest \n",
    "for all flowlines upstream of (and including) the flowline (COMID) of interest\n",
    "\n",
    "This dataset is accessible via R, but we must know the name of the attributes in which we are\n",
    "interested. Attribute names can be found Variable names in the metadata_table.tsv on ScienceBase \n",
    "(https://www.sciencebase.gov/catalog/item/5669a79ee4b08895842a1d47). In general, we are\n",
    "interested in the tot_xxx attributes, but sometimes we want the cat_xxx data for an entire\n",
    "watershed, so we can calculate medians and other distribution data.\n",
    "\n",
    "This section is generally organized by groups of related features. \n",
    "\n",
    "### Hydrology\n",
    "\n",
    "#### Runoff and groundwater recharge\n",
    "\n",
    "Infiltration-Excess Overland Flow, Saturation-Excess Overland Flow, \n",
    "Average Annual Runoff, RUSLE R-factor, Topographic Wetness Index,\n",
    "Groundwater Recharge, Contact Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f750b",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Infiltration-Excess Overland Flow \n",
    "#### Values represent the mean percentage of total streamflow that is comprised\n",
    "#### of infiltration-excess overland flow\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/56f974e2e4b0a6037df06b55\n",
    "\n",
    "### Saturation-Excess Overland Flow\n",
    "#### Values represent the mean % of total streamflow that is derived from\n",
    "#### saturation-excess overland flow\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/56f97acbe4b0a6037df06b6a\n",
    "\n",
    "### Estimated Average annual runoff 1971-2000\n",
    "#### Measures the average annual runoff (in mm) based on flow measured at streamgages \n",
    "#### More here: https://www.sciencebase.gov/catalog/item/578f8ad8e4b0ad6235cf6e43\n",
    "\n",
    "### RUSLE R-factor\n",
    "#### This is the mean annual average for the rainfall and runoff factor in the\n",
    "#### Revised Universal Soil Loss Equation (RUSLE) as derived from PRISM data\n",
    "#### Units are ft-ton force-inch/acre-hour\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/573b6028e4b0dae0d5e3ae16\n",
    "\n",
    "### Topographic Wetness Index (TWI)\n",
    "#### This is a steady used to predict areas susceptible to saturated land surfaces \n",
    "#### and areas that carry the potential to produce overland flow\n",
    "#### Units are ln(m) \n",
    "#### Areas with higher topographic wetness index values are likely to be wetter relative \n",
    "#### to areas with lower values\n",
    "#### Smaller values of the TWI indicate less potential for development of ponding\n",
    "#### Values range range from less than 1 (dry cells) to greater than 20 (wet cells).\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/56f97be4e4b0a6037df06b70\n",
    "\n",
    "### Groundwater Recharge\n",
    "#### 30 year (1951-1980) mean annual natural groundwater recharge in mm/yr\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/56f97577e4b0a6037df06b5a\n",
    "\n",
    "### Contact Time\n",
    "#### Time it takes for water to drain along subsurface flows paths to the stream\n",
    "#### Derived from methods outlined in Wolock et al (1997)\n",
    "#### Contact time is computed from basin topography, soil porosity, and \n",
    "#### soil hydraulic conductivity.\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/56f96fc5e4b0a6037df06b12\n",
    "\n",
    "### Water Table Elevation\n",
    "#### Average depth to the water table relative to the land surface\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/56f97456e4b0a6037df06b50\n",
    "\n",
    "##### Make a list of variables to retrieve\n",
    "\n",
    "run_gw_chars <- c(\"TOT_IEOF\", ### Infiltration-Excess Overland Flow\n",
    "                  \"TOT_SATOF\", ### Saturation-Excess Overland Flow\n",
    "                  \"TOT_RUN7100\", ### Runoff from gages\n",
    "                  \"TOT_RF7100\", ### RUSLE R-factor \n",
    "                  \"TOT_TWI\", ### Topographic-Wetness Index\n",
    "                  \"TOT_RECHG\", ### Groundwater Recharge\n",
    "                  \"TOT_CONTACT\", ### Contact time\n",
    "                  \"TOT_EWT\" ### Water table elevation\n",
    "                  )\n",
    "\n",
    "##### Retrieve the variables \n",
    "\n",
    "run_gw <- get_catchment_characteristics(run_gw_chars, \n",
    "                                        start_comid_df$comid)\n",
    "\n",
    "##### Format characteristic table in a wide format\n",
    "##### And join to table with tributary names\n",
    "\n",
    "run_gw <- run_gw %>%\n",
    "  pivot_nhd_chars_wide(comids_df = start_comid_df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035bbf05",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Snow\n",
    "\n",
    "Mean Annual Snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee3432",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Mean annual snow as a percent of total precip, 1905-2002\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57053dc5e4b0d4e2b756c117\n",
    "\n",
    "##### Retrieve the variables \n",
    "\n",
    "snow <- get_catchment_characteristics(\"TOT_PRSNOW\", \n",
    "                                        start_comid_df$comid)\n",
    "\n",
    "##### Format the data (using our custom function)\n",
    "\n",
    "snow <- snow %>%\n",
    "  pivot_nhd_chars_wide(comids_df = start_comid_df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232205e",
   "metadata": {},
   "source": [
    "#### Precipitation total & intensity\n",
    "\n",
    "Here, we are going to download various precipitation metrics that we think govern\n",
    "constituent dynamics, including mean annual precip, maximum annual precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba1957",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "\n",
    "#### Mean annual precip,in mm, 1971-2000, derived from PRISM data\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/573b70a7e4b0dae0d5e3ae85\n",
    "\n",
    "mean_annual_precip <- get_catchment_characteristics(\"TOT_PPT7100_ANN\",\n",
    "                                                    all_sites_metadata_all$comid)\n",
    "\n",
    "\n",
    "\n",
    "mean_annual_precip <- mean_annual_precip %>%\n",
    "  pivot_nhd_chars_wide(comids_df = all_sites_metadata_all)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "#### Average Maximum monthly precipitation, units mm/month, \n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57040af4e4b0328dcb82842b\n",
    "\n",
    "mean_max_monthly_precip <- get_catchment_characteristics(\"TOT_MAXP6190\",\n",
    "                                                    all_sites_metadata_all$comid) %>%\n",
    "    pivot_nhd_chars_wide(comids_df = all_sites_metadata_all)\n",
    "\n",
    "mean_max_monthly_precip <- mean_max_monthly_precip %>%\n",
    "  rename(mean_max_monthly_precip_mm = tot_maxp6190)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "#### Mean Annual Daily Precip Intensity, mm/day, 1981-2010\n",
    "#### Derived from 1-km resolution data downloaded from DAYMET\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/6398cad0d34e0de3a1f0d75e\n",
    "\n",
    "mean_annual_precip_intensity <- get_catchment_characteristics(\"TOT_Intensity\",\n",
    "                                                    all_sites_metadata_all$comid) %>%\n",
    "    pivot_nhd_chars_wide(comids_df = all_sites_metadata_all)\n",
    "\n",
    "mean_annual_precip_intensity <- mean_annual_precip_intensity %>%\n",
    "  rename(mean_annual_precip_intensity_mm_d = 2)\n",
    "\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f68cf9",
   "metadata": {},
   "source": [
    "### Hydrography & Geomorphology\n",
    "\n",
    "#### Average Distance to Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05a22b",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### The average distance to a flow line (stream network) from any given land cell\n",
    "### for each NHDPlus V2 catchment. More about how this was derived can be found here: \n",
    "### https://www.sciencebase.gov/catalog/item/5d1a1dfbe4b0941bde6025d2\n",
    "\n",
    "#### For whatever reason, this is not accessible directly from R\n",
    "#### So we will have to download the data file using the ScienceBase API\n",
    "#### And the sbtools package\n",
    "\n",
    "##### To do, use our custom function\n",
    "\n",
    "dist_to_stream <- read_direct_from_sb(sci_base_id = \"5d1a1dfbe4b0941bde6025d2\")\n",
    "\n",
    "##### Join to table with tributary names\n",
    "##### And format the table as we want\n",
    "dist_to_stream <- dist_to_stream %>%\n",
    "  as_tibble() %>%\n",
    "  rename_all(~tolower(.)) %>%\n",
    "  dplyr::select(!nodata) %>%\n",
    "  inner_join(start_comid_df, .,\n",
    "             by = \"comid\") %>%\n",
    "  dplyr::select(!comid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664ed31",
   "metadata": {},
   "source": [
    "#### Sinuosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0d27e",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Sinuosity was calculated for each reach \n",
    "#### And no accumulation was done\n",
    "#### Therefore, we need to approach this calculation a bit different\n",
    "#### As we are interested in watershed-scale sinuousity rather than just\n",
    "#### at the outlet reach\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57976a0ce4b021cadec97890\n",
    "\n",
    "##### First, download the sinuousity for all COMIDs\n",
    "\n",
    "sinuosity <- get_catchment_characteristics(\"CAT_sinuosity\", \n",
    "                                        comids_by_basin$comid)\n",
    "\n",
    "##### Then, pivot the dataframe to wide format and \n",
    "##### calculate the mean sinuosity within each tributary watershed\n",
    "\n",
    "sinuosity <- sinuosity %>%\n",
    "  pivot_nhd_chars_wide(comids_df = comids_by_basin) %>%\n",
    "  dplyr::group_by(tributary) %>%\n",
    "  summarise(mean_sin = mean(cat_sinuosity))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224db1a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Bankfull width and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31f8bc",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### An estimation of bankfull width based regression equations in \n",
    "#### Bieger et al. 2015 (https://doi.org/10.1111/jawr.12282)\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/5cf02bdae4b0b51330e22b85\n",
    "\n",
    "#### For whatever reason, this is not accessible directly from R\n",
    "#### So we will have to download the data file using the ScienceBase API\n",
    "#### These are also not \"accumulated\" so we will have to manually calculate\n",
    "#### the mean bankfull width for each catchment\n",
    "\n",
    "##### To do, use our custom function\n",
    "\n",
    "hyd_geometry <- read_direct_from_sb(sci_base_id = \"5cf02bdae4b0b51330e22b85\")\n",
    "\n",
    "##### Then calculate the mean hydraulic geometry for each watershed \n",
    "##### And reformat the table a bit better\n",
    "\n",
    "hyd_geometry <- hyd_geometry %>%\n",
    "  as_tibble() %>%\n",
    "  rename_all(~tolower(.)) %>%\n",
    "  filter(bankfull_width != -9999) %>%\n",
    "  dplyr::select(comid, bankfull_width, bankfull_depth) %>%\n",
    "  inner_join(., comids_by_basin %>%\n",
    "               mutate(comid = as.integer(comid)),\n",
    "             by = \"comid\") %>%\n",
    "  dplyr::group_by(tributary) %>%\n",
    "  summarise(mean_bf_width = mean(bankfull_width),\n",
    "            mean_bf_depth = mean(bankfull_depth))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1edc7c",
   "metadata": {},
   "source": [
    "### Geology & Soils\n",
    "\n",
    "#### Soil characteristics\n",
    "\n",
    "Soil Hydrologic Groups, Soil Phosphorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb683a0e",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Download data regarding the percentage of soils \n",
    "#### of Hydrologic Group A and B, which are well to moderately-drained soils with high\n",
    "#### infiltration capacity and typically of coarse to moderately coarse texture\n",
    "#### (sands and gravel)\n",
    "#### These data are derived from the SSURGO/STATSGO soils database compiled by the \n",
    "#### US Dept. of Agriculture\n",
    "#### Units are in percent\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/5728d93be4b0b13d3918a99f\n",
    "\n",
    "#### Soil phosphorus\n",
    "#### Estimated distribution of phosphorus in soil A and C horizons\n",
    "#### From the source publication Terziotti, 2019 \n",
    "#### More here: https://www.sciencebase.gov/catalog/item/58580f18e4b0e40e53c237a5\n",
    "\n",
    "soils_vars <- c(\"TOT_HGA\", ### Hyd. Group A  \n",
    "                \"TOT_HGB\", ### Hyd Group B\n",
    "                \"TOT_SOILS_A_P\", ### Phos in Group A\n",
    "                \"TOT_SOILS_C_P\" ### Phos is Group C\n",
    "                )\n",
    "\n",
    "##### Get the data\n",
    "\n",
    "soils <- get_catchment_characteristics(varname = soils_vars,\n",
    "                                          start_comid_df$comid)\n",
    "\n",
    "\n",
    "##### Pivot wider and combine into one variable that represents\n",
    "##### the extent of good drainage in a basin\n",
    "\n",
    "soils <- soils %>%\n",
    "  pivot_nhd_chars_wide(comids_df = start_comid_df) %>%\n",
    "  mutate(pct_ab_soils = tot_hga + tot_hgb) %>%\n",
    "  mutate(tot_p_ac_soils = tot_soils_a_p + tot_soils_c_p) %>%\n",
    "  dplyr::select(tributary, pct_ab_soils, tot_p_ac_soils) %>%\n",
    "  inner_join(., final_streamstat_features %>%\n",
    "               dplyr::select(!el1200),\n",
    "             by= \"tributary\") %>%\n",
    "  mutate(pct_p_ac_soils = tot_p_ac_soils/drnarea_km2) %>%\n",
    "  dplyr::select(tributary, pct_ab_soils, pct_p_ac_soils)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d895c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Surficial Geology and Bedrock Geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365062f1",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Surficial Geology compiled from the USGS map database for surficial \n",
    "#### materials in the United States (https://pubs.usgs.gov/ds/425/)\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57d8529ee4b090824ff9ac91\n",
    "#### Bedrock Geology compiled from the generalized geologic map of the United States\n",
    "#### (https://pubs.usgs.gov/atlas/geologic/)\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/5703f6b5e4b0328dcb826d06\n",
    "#### Units are all in percent\n",
    "\n",
    "#### There are many categories of both surficial and bedrock, \n",
    "#### not all of which are present in every watershed. \n",
    "#### Unfortunately, they are all under different variable names\n",
    "#### with a slightly esoteric naming& numbering scheme, and so we must know beforehand\n",
    "#### all the variable names to download directly from R using the nhdTools functions\n",
    "#### This is, in short, a bit of a pain\n",
    "#### To this most efficiently, it makes sense to find all the variable names\n",
    "#### Within the metadata\n",
    "#### Luckily, they all contain the string \"SOLLER\" (surficial) or \"BUSHREED\" (bedrock)\n",
    "\n",
    "##### First, we need to download the metadata from ScienceBase\n",
    "##### Here: https://www.sciencebase.gov/catalog/item/5669a79ee4b08895842a1d47\n",
    "\n",
    "\n",
    "overall_sb_item <- sbtools::item_get(\"5669a79ee4b08895842a1d47\")\n",
    "\n",
    "##### We are interested in the metadata, so see all the files associated with this\n",
    "##### ScienceBase item and extract the relevant one\n",
    "\n",
    "for(i in 1:length(overall_sb_item$files)) {\n",
    "  \n",
    "  print(i)\n",
    "  \n",
    "  sb_file_names[[i]] <- overall_sb_item$files[[i]]$name \n",
    "  \n",
    "}\n",
    "\n",
    "print(sb_file_names)\n",
    "\n",
    "##### We can see that sb_file_names[[5]] is the metadata file we want\n",
    "##### So let's now download that file and import it to R\n",
    "\n",
    "nhd_chars_metadata_path <- sbtools::item_file_download(sb_id = x, names = sb_file_names[[5]],\n",
    "                                   destinations = file.path(tempdir(), sb_file_names[[5]]),\n",
    "                                   overwrite_file = TRUE)\n",
    "\n",
    "nhd_chars_metadata <- read_tsv(nhd_chars_metadata_path)\n",
    "\n",
    "\n",
    "##### Extract the relevant variable names\n",
    "##### Which here are those relating to Soller surficial geology\n",
    "##### And those relating to Reed & Bush bedrock geology\n",
    "\n",
    "surf_vars <- nhd_chars_metadata %>%\n",
    "  filter(str_detect(ID, \"TOT_SOLLER\"))\n",
    "\n",
    "bedrock_vars <- nhd_chars_metadata %>%\n",
    "  filter(str_detect(ID, \"TOT_BUSHREED\"))\n",
    "\n",
    "##### Now download the characteristics for our watersheds \n",
    "\n",
    "surf_geology <- get_catchment_characteristics(surf_vars$ID, \n",
    "                                              start_comid_df$comid)\n",
    "\n",
    "##### Make wider and remove types not found in any watershed in\n",
    "##### the Lake Champlain Basin\n",
    "\n",
    "surf_geology <- surf_geology %>%\n",
    "  pivot_nhd_chars_wide(comids_df = start_comid_df) %>%\n",
    "  dplyr::select(tributary,\n",
    "                where(~is.numeric(.) && \n",
    "                        sum(.) != 0) \n",
    "                ) %>% ### Remove types that aren't present anywhere\n",
    "  dplyr::select(!tot_soller_999) ### Remove water, which we already have from land cover \n",
    "\n",
    "\n",
    "##### Now download bedrock geology\n",
    "\n",
    "bedrock_geology <- get_catchment_characteristics(bedrock_vars$ID, \n",
    "                                        start_comid_df$comid)\n",
    "\n",
    "##### Make wider and remove types not found in any watershed in\n",
    "##### the Lake Champlain Basin\n",
    "\n",
    "bedrock_geology <- bedrock_geology %>%\n",
    "  pivot_nhd_chars_wide(comids_df = start_comid_df) %>%\n",
    "  dplyr::select(tributary,\n",
    "                where(~is.numeric(.) && \n",
    "                        sum(.) != 0) \n",
    "                ) %>% ### Remove types that aren't present anywhere\n",
    "  dplyr::select(!tot_bushreed_8) ### Remove water, which we already have from land cover \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bba14",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Other Geologic Attributes\n",
    "\n",
    "Lithologic Hydraulic Conductivity, Geologic Phosphorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8cd9c",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Get data related to permeability (really, lithological hydraulic conductivity)\n",
    "#### From Olson and Hawkins, 2014\n",
    "#### Units are in micrometers/second\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/5703e35be4b0328dcb825562\n",
    "\n",
    "#### Also get data related to loading of phosphorus from geologic materials\n",
    "#### Source data is from Nardi, 2014\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/5b297a25e4b059207627a3ad\n",
    "#### Data are in ppm\n",
    "\n",
    "\n",
    "\n",
    "##### Get data\n",
    "\n",
    "other_geol <- get_catchment_characteristics(varname = c(\"TOT_OLSON_PERM\",\n",
    "                                                        \"TOT_PMAP\"),\n",
    "                                          start_comid_df$comid)\n",
    "\n",
    "##### Put into good format\n",
    "other_geol <- other_geol %>%\n",
    "  pivot_nhd_chars_wide(comids_df = start_comid_df) %>%\n",
    "  inner_join(., final_streamstat_features %>%\n",
    "               dplyr::select(!el1200),\n",
    "             by= \"tributary\") %>%\n",
    "  mutate(tot_pmap_pct = tot_pmap/drnarea_km2) %>% ### Normalize phos loading by watershed area\n",
    "  dplyr::select(tributary, tot_olson_perm, tot_pmap_pct)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47710607",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afb78461",
   "metadata": {},
   "source": [
    "### Topography\n",
    "\n",
    "#### Stream & Basin Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9de2f6",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### This is slope of the streams in a given watershed AND\n",
    "#### This is the slope of the entire basin (not just the streams)\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57976a0ce4b021cadec97890\n",
    "\n",
    "##### Get data\n",
    "\n",
    "topo <- get_catchment_characteristics(c(\"TOT_BASIN_SLOPE\",\n",
    "                                             \"TOT_STREAM_SLOPE\"),\n",
    "                                        start_comid_df$comid)\n",
    "\n",
    "\n",
    "##### Format \n",
    "\n",
    "topo <- topo %>%\n",
    "    pivot_nhd_chars_wide(comids_df = start_comid_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972edb84",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Anthropogenic factors and modifications\n",
    "\n",
    "Fertilizer application, septic systems, wastewater, dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bd796",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Phosphorus Fertilzer Application 1997\n",
    "#### Source data is a 1997 USGS publication by Ruddy et al\n",
    "#### Units are kg/km2\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57053749e4b0d4e2b756b969\n",
    "\n",
    "### Phosphorus Fertilizer & Manure Application 2012\n",
    "#### Source data is a 2012 USGS publication for manure (Gronberg et al., 2017)\n",
    "#### And a 2017 USGS publication for phos from fertilizer (Brakebill et al., 2017)\n",
    "#### Units are in kilograms\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/59038bc8e4b0e862d2311aef\n",
    "\n",
    "### Dams \n",
    "#### Source data is from the Army Corps of Engineers\n",
    "#### Data are both the number of major * minor dams and the storage capacity\n",
    "#### Per USGS metadata, tot_ndams_2010 = Accumulated number of dams based on total upstream accumulation.\n",
    "#### TOT_MAJOR: Accumulate number of major dams based on total upstream accumulation.\n",
    "#### TOT_NORM_STORAGE: accumulated normal dam storage (in acre-feet) defined as the total storage space \n",
    "#### in a reservoir below the normal retention level\n",
    "\n",
    "### Wastewater\n",
    "#### The number and desnity of major National Pollutant Discharge Elimination System sites\n",
    "#### Source data is from the EPA\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57c9d89ce4b0f2f0cec192da\n",
    "\n",
    "### Population Density\n",
    "#### Data from block-level population density rasters created by the Census Bureau\n",
    "#### We want population density for 2010, because it hasn't changed much in this basin\n",
    "#### Units are persons/km2\n",
    "#### More here: https://www.sciencebase.gov/catalog/item/5728f746e4b0b13d3918aa1e\n",
    "#### We want both mean upstream population and population at the outlet\n",
    "#### And then we will also calculate maximum upstream population\n",
    "\n",
    "### Road density\n",
    "#### Data is for road density by road type\n",
    "#### We are interested in the density of roads of all types \n",
    "#### More here: https://www.sciencebase.gov/catalog/item/57976a0ce4b021cadec97890\n",
    "\n",
    "##### Declare the variables we are interested in\n",
    "\n",
    "anthro_vars <- c(\"TOT_P97\", ### Phos 1997\n",
    "                 \"TOT_TP2012\", ### Phos 2012\n",
    "                 \"TOT_NDAMS2010\", ### Number of dams 2010\n",
    "                 \"TOT_NORM_STORAGE2010\", ### Reservoir storage 2010\n",
    "                 \"TOT_NPDES_MAJ\", ### Wastewater outlets\n",
    "                 \"TOT_POPDENS10\", ### Pop density\n",
    "                 \"CAT_POPDENS10\", ### Pop density\n",
    "                 \"TOT_TOTAL_ROAD_DENS\" ### Road density \n",
    "                 )\n",
    "\n",
    "##### Get the data\n",
    "\n",
    "anthro_chars <- get_catchment_characteristics(anthro_vars, \n",
    "                                        start_comid_df$comid)\n",
    "\n",
    "##### Format the data\n",
    "\n",
    "anthro_chars <- anthro_chars %>%\n",
    "    pivot_nhd_chars_wide(comids_df = start_comid_df)\n",
    "\n",
    "\n",
    "##### Now also get population data for every COMID in each watershed\n",
    "max_pop_dens <- get_catchment_characteristics(\"CAT_POPDENS10\", \n",
    "                                        comids_by_basin$comid)\n",
    "\n",
    "##### And then calculate maximum population density in each watershed\n",
    "max_pop_dens <- max_pop_dens %>%\n",
    "  pivot_nhd_chars_wide(comids_df = comids_by_basin) %>%\n",
    "  dplyr::group_by(tributary) %>%\n",
    "  summarise(max_popdens10 = max(cat_popdens10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba6c00",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Combine together\n",
    "\n",
    "Combine all the watershed characteristics we have calculated/acquired into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615fd37",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "### List all characteristics we've gathered \n",
    "\n",
    "all_chars <- list(final_streamstat_features,\n",
    "                  lulc_all,\n",
    "                  basin_drainage_density,\n",
    "                  pct_by_type,\n",
    "                  pct_orders,\n",
    "                  relief_m,\n",
    "                  hacks,\n",
    "                  flashiness,\n",
    "                  flow_anomaly,\n",
    "                  tile_percent_by_basin,\n",
    "                  run_gw,\n",
    "                  dist_to_stream,\n",
    "                  sinuosity,\n",
    "                  hyd_geometry,\n",
    "                  soils,\n",
    "                  surf_geology,\n",
    "                  bedrock_geology,\n",
    "                  other_geol,\n",
    "                  topo,\n",
    "                  anthro_chars,\n",
    "                  max_pop_dens)\n",
    "\n",
    "\n",
    "#### Now combine 'em together\n",
    "\n",
    "watershed_chars <- purrr::reduce(all_chars,\n",
    "                                 full_join,\n",
    "                                 by = \"tributary\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a69713",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "eval,name,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
