{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b783298e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"00.4_forecast_downloads\"\n",
    "author: \"JTK\"\n",
    "date: \"2025-05-19\"\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e9224",
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cddb19",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Housekeeping\n",
    "\n",
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# General\n",
    "\n",
    "require(here)\n",
    "\n",
    "# Spatial\n",
    "\n",
    "require(terra)\n",
    "require(sf)\n",
    "\n",
    "# Data mgmt\n",
    "\n",
    "require(tidyverse)\n",
    "require(glue)\n",
    "require(tidync)\n",
    "\n",
    "# Data viz\n",
    "\n",
    "require(ggthemes)\n",
    "require(tmap)\n",
    "require(mapview)\n",
    "\n",
    "# Hydrography\n",
    "\n",
    "require(nhdplusTools)\n",
    "\n",
    "# NWM\n",
    "\n",
    "require(nwmTools)\n",
    "\n",
    "### Data mgmt\n",
    "\n",
    "require(tidyverse)\n",
    "require(glue)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cad529",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "## Get NWM forecasts\n",
    "\n",
    "### Functions \n",
    "\n",
    "#### New NWM filter because things were broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6140769",
   "metadata": {
    "name": "nwm_filter2"
   },
   "outputs": [],
   "source": [
    "\n",
    "### Need to have this because it's i\n",
    "\n",
    "\n",
    "# validate = function(complete, field, value){\n",
    "#   \n",
    "#   if(field %in% names(complete) & !is.null(value)){\n",
    "#     opts = unique(complete[[field]])\n",
    "# \n",
    "#     \n",
    "#     if(any(grepl(value, opts))){\n",
    "#       return(filter(complete, grepl(!!value, get(field))))\n",
    "#     } else {\n",
    "#       stop(glue(\"{value} not a valid {field}. Choose from: {paste(opts, collapse = ', ')}\"))\n",
    "#     }\n",
    "#   } else {\n",
    "#     return(complete)\n",
    "#   }\n",
    "# }\n",
    "\n",
    "### Re-write the nwm_filter function internal to nwmTools\n",
    "### because there is a) a typo and b) the date formats in the nwm_data\n",
    "### reference tibble have changed to non-standard formats, which the base R functions\n",
    "### used for various date operations cannot parse \n",
    "\n",
    "nwm_filter2 = function(source, version = NULL, config = NULL, ensemble = NULL, \n",
    "                      output= NULL, domain = NULL, date = NULL){\n",
    "  \n",
    "  startDate <- endDate <- NULL\n",
    "  \n",
    "  # Source ------------------------------------------------------------------\n",
    "  \n",
    "  meta = validate(nwmTools::nwm_data, \"source\", source) %>% \n",
    "         validate(\"domain\", domain) %>% \n",
    "         validate(\"version\", version) %>% \n",
    "         validate(\"output\", output) %>% \n",
    "         validate(\"config\", config) %>% \n",
    "         validate(\"ensemble\", ensemble) \n",
    "  \n",
    "\n",
    "  # Date ------------------------------------------------------------------ \n",
    "\n",
    "  if(!is.null(date)){\n",
    "    \n",
    "    meta = meta %>% \n",
    "      mutate(startDate = ifelse(startDate == \"..\", \n",
    "                                \"1900-01-01\",\n",
    "                                parse_date_time(startDate, \"mdy\")),\n",
    "             endDate = ifelse(endDate == \"..\",  as.character(Sys.Date()), endDate))\n",
    "    \n",
    "    tmp = filter(meta, as.POSIXct(date, tz = \"UTC\") >= as.POSIXct(startDate, tz = \"UTC\"),\n",
    "                 as.POSIXct(date, tz = \"UTC\") <= as.POSIXct(endDate, tz = \"UTC\"))\n",
    "    \n",
    "    if(nrow(tmp) == 0){ stop(\"date not within allowed range: \", meta$startDate, \"--\", meta$endDate) } else { meta = tmp }\n",
    "  }\n",
    "  \n",
    "  \n",
    "  if(nrow(meta) > 1){warning('More then one viable source found...', call. = FALSE)}\n",
    "  \n",
    "  meta\n",
    "}\n",
    "\n",
    "\n",
    "environment(nwm_filter2) <- environment(get_gcp_urls)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e616d",
   "metadata": {},
   "source": [
    "#### Generate the download URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03b27e",
   "metadata": {
    "name": "get_gcp_urls2"
   },
   "outputs": [],
   "source": [
    "### We need to re-write some functions from the nwmTools package\n",
    "### Here we re-write the geturls command to better reflect \n",
    "### the URLS of the medium-term forecasts (the built-in function had syntax\n",
    "### more reflective of the short-term urls)\n",
    "\n",
    "get_gcp_urls2 <- function (config = \"short_range\", \n",
    "                                domain = \"conus\", date, hour = \"00\",\n",
    "                                minute = \"00\", num, ensemble = NULL, \n",
    "                                output = \"channel_rt\") \n",
    "{\n",
    "    meta = nwm_filter2(source = \"gcp\", version = NULL, config = config, \n",
    "        date = date, ensemble = ensemble, output = output, domain = domain)\n",
    "    \n",
    "    dates = seq.POSIXt(as.POSIXlt(paste(date, hour, minute), \n",
    "        tz = \"UTC\"), length.out = num+1, by = \"1 hour\")\n",
    "    YYYYDDMM = rep(format(as.POSIXct(date, tz = \"UTC\"), \"%Y%m%d\"), num)\n",
    "    forward = sprintf(\"%03d\", seq(1, num))\n",
    "    urls = glue(meta$http_pattern, YYYYDDMM = YYYYDDMM, config = meta$config[1], \n",
    "        HH = hour, foward = forward, output = meta$output[1], \n",
    "        domain = meta$domain, ensemble = meta$ensemble[1], prefix = meta$prefix[1])\n",
    "    dates = ymd_hm(paste0(date[1], hour, \"00\")) + hours(1:(num))\n",
    "    data.frame(dateTime = dates, urls = urls, output = output)\n",
    "} \n",
    "\n",
    "#### Must do this so that some functions internal to nwmTools can be used\n",
    "\n",
    "environment(get_gcp_urls2) <- environment(get_gcp_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b2b8d",
   "metadata": {},
   "source": [
    "#### Get the timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a603b",
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "get_timeseries3"
   },
   "outputs": [],
   "source": [
    "### This function dowloads archived NWM forecasts from the Google Bucket\n",
    "### where they are stored. It utilizes cloud-based operations to extract\n",
    "### the reaches we are interested in BEFORE downloading the entire NWM file\n",
    "### to our local machine. In this way we avoid downloading giant files that \n",
    "### contain largely extraneous data. This is perhaps the more \"correct\" way to \n",
    "### do this. \n",
    "### Note that this is a rewrite of the nwmTools get_timeseries function within \n",
    "### Mike Johnson's nwmTools package. We are heavily indebted to the great work\n",
    "### Mike has done. Thanks Mike!!!!!! (https://github.com/mikejohnson51)\n",
    "### Also, *******It is NOT FAST*******\n",
    "### Perhaps it could be rewritten for efficiency, but hey, it works\n",
    "\n",
    "\n",
    "\n",
    "get_timeseries3 <- function(urls = NULL, \n",
    "                            ids = NULL,\n",
    "                            outfile = NULL,\n",
    "                            index_id = \"feature_id\",\n",
    "                            varname = \"streamflow\"\n",
    "\n",
    "                            \n",
    ") {\n",
    "  \n",
    "            #### Get values function #######################################################\n",
    "            get_values <- function(url, var, ind = NULL) {\n",
    "                        v = suppressWarnings(values(terra::rast(glue(\"{url}://{var}\"))))\n",
    "                        \n",
    "                        if (!is.null(ind)) {\n",
    "                            v = v[ind]\n",
    "                        }\n",
    "                        else {\n",
    "                            v = v\n",
    "                        }\n",
    "                        \n",
    "                        return(v)\n",
    "            }\n",
    "            ################################################################################\n",
    "\n",
    "\n",
    "### Get lead time from URL \n",
    "### And get init_date from URL\n",
    "            \n",
    "lead_time <- str_extract(str_extract(urls, \"f\\\\d+\"), \"\\\\d+\")\n",
    "\n",
    "init_date <- as_date(str_extract(str_extract(urls, \".\\\\d+\"), \"\\\\d+\"))\n",
    "\n",
    "member <- str_extract(urls, \"mem\\\\d+\")\n",
    "\n",
    "#### Little updater\n",
    "\n",
    "print(paste(\"Downloading\", init_date, lead_time, member, \" \"))\n",
    "\n",
    "### First, set up a URL that turns the netCDF on the Google Bucket\n",
    "### Into a HD5 file\n",
    "### And utilizes external pointer (vsicurl) to access file \"on the fly\"\n",
    "\n",
    "nwm_url2 <- glue(\"HDF5:\\\"/vsicurl/{urls}\\\"\")\n",
    "\n",
    "\n",
    "### Now get the feature_ids (comids) from the file\n",
    "\n",
    "all_ids <- get_values(url = nwm_url2, index_id)\n",
    "\n",
    "\n",
    "### Now find the indexes of our comids (reaches) of interest\n",
    "### In the file that contains all the comids\n",
    "### The thinking here is that the index of a given comid in the feature_id \"layer\"\n",
    "### Should be the same index of where the streamflow value corresponding to that comid\n",
    "### Is located\n",
    "### We need to put in a bunch of safety if-else statements to keep from breaking \n",
    "### if the file is not found for whatever reason\n",
    "\n",
    " \n",
    "   if (!is.null(index_id)) {\n",
    "     \n",
    "     \n",
    "     \n",
    "      all_ids = get_values(url = nwm_url2, index_id)\n",
    "                \n",
    "     ### If no COMIDs are entered, return every streamflow ID\n",
    "     ### So all 2.7 million reaches in the NWM\n",
    "     ### But if particular COMIDs are desired\n",
    "     ### Find the index of those COMIDs\n",
    "     ### in the feature_id \"layer\"\n",
    "     \n",
    "        if (is.null(ids)) {\n",
    "            ind = NULL\n",
    "            ids = all_ids\n",
    "        }\n",
    "     \n",
    "        else {\n",
    "         \n",
    "                ind <- which(all_ids %in% ids)\n",
    "            \n",
    "        }\n",
    "   }\n",
    "\n",
    "    else {\n",
    "      \n",
    "        ind = NULL\n",
    "        ids = NULL\n",
    "        \n",
    "    } ### Returns null if error in reading files\n",
    "\n",
    "\n",
    "\n",
    "#### Now let's get streamflow\n",
    "#### It comes in without decimals (not sure why)\n",
    "\n",
    "q_values <- get_values(nwm_url2, varname, ind)\n",
    "\n",
    "q_values <- q_values/100\n",
    "\n",
    "#### And time \n",
    "#### Which we need to multiply by 60\n",
    "#### to turn into \"true\" POSICxt time (seconds from 1970-01-01)\n",
    "\n",
    "nwm_time <- get_values(nwm_url2, \"time\")[1]*60\n",
    "\n",
    "\n",
    "#### Now, we have to actually extract the feature_id in the same way as we did\n",
    "#### for discharge\n",
    "#### This gets us a vector ordered in the same order as our discharge vector\n",
    "#### Without this, we will bind things that our in different orders\n",
    "#### and our final output dataframe will be meaningless \n",
    "#### THIS IS EXTREMELY IMPORTANT\n",
    "\n",
    "comids <- get_values(nwm_url2, index_id, ind)\n",
    "\n",
    "### Bind the time, COMIDs, and modeled streamflow values into one tibble\n",
    "### Make into a tibble\n",
    "\n",
    "streamflow_by_reach <- tibble(comid = comids, \n",
    "                              init_date = init_date, \n",
    "                              lead_time = lead_time,\n",
    "                              member = member,\n",
    "                              predict_dateTime = as_datetime(nwm_time),\n",
    "                              modeled_q_cms = q_values\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "### And write that to file \n",
    "\n",
    "write_csv(streamflow_by_reach, outfile,\n",
    "          append = TRUE,\n",
    "          col_names = !file.exists(outfile),\n",
    "          )\n",
    "\n",
    "\n",
    "#return(streamflow_by_reach)\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d962a7",
   "metadata": {},
   "source": [
    "####### Note that this uses functions from the turb-forecasting 00_functions script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### First, get the dates that we are interested in\n",
    "#### We only want to download forecasts from v2.1 of the NWM\n",
    "#### Which was operational from from 21 April 2021 to 20 September 2023\n",
    "\n",
    "download_dates <- tibble(date = seq.Date(from = as.Date(\"2024-06-15\"), \n",
    "                                            to = as.Date(\"2024-07-15\"), \n",
    "                                            by = \"day\")) %>%\n",
    "  .$date\n",
    "\n",
    "#### Now generate the Google Bucket URLs where each of those forecasts is archived\n",
    "#### Note that the medium-term forecast is initialized every 6 hours, and forecasts\n",
    "#### out 204 hrs (8.5 days) in the future\n",
    "#### We only want to download one daily initialization, which is that made at \n",
    "#### midnight UTC\n",
    "#### There are also six members of the medium-term NWM\n",
    "#### We want to download them all\n",
    "\n",
    "#### URLS for V2.1 ######################################################################\n",
    "#########################################################################################\n",
    "\n",
    "#### Make some empty lists to save things\n",
    "\n",
    "urls_per_member_v21 <- list()\n",
    "\n",
    "urls_all_days_mt_v21 <- list()\n",
    "\n",
    "#### Loop over all dates\n",
    "\n",
    "for(j in 1:length(download_dates)) {\n",
    "  \n",
    "  forecast_date_mt <- download_dates[j]\n",
    "  \n",
    "  #### Loop over each member\n",
    "  \n",
    "  for(i in seq(1, 6, 1)){\n",
    "    \n",
    "        #### Print progress\n",
    "  \n",
    "        print(paste(forecast_date_mt, \"_ ens\", i ))\n",
    "        \n",
    "        #### Generate URLs using out rewrite of the nwmTools function\n",
    "      \n",
    "        urls_v21 <- get_gcp_urls2(domain = \"conus\",\n",
    "          output = \"channel_rt\",\n",
    "          config = \"medium_range\",\n",
    "          ensemble = i,\n",
    "          date = forecast_date_mt,\n",
    "          hour = \"00\",\n",
    "        minute = \"00\",\n",
    "        num = 204)\n",
    "        \n",
    "        #### Clean-up the urls\n",
    "        \n",
    "        urls_v21 <- urls_v21 %>%\n",
    "          #mutate(urls = sub(\"(.*)f \", \"\\\\1f0\\\\2\", urls)) %>%\n",
    "          mutate(init_time = 0) %>%\n",
    "          mutate(init_date = forecast_date_mt) %>%\n",
    "          mutate(member = paste0(\"mem\", i))\n",
    "        \n",
    "        #### Store each member for a given date\n",
    "        \n",
    "        urls_per_member_v21[[i]] <- urls_v21\n",
    "    \n",
    "  }\n",
    "  \n",
    "  #### Store all members for each date\n",
    "  \n",
    "  urls_all_days_mt_v21[[j]] <- urls_per_member_v21\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Join all the urls together\n",
    "\n",
    "all_urls_mt <- bind_rows(urls_all_days_mt_v21) %>%\n",
    "  mutate(predict_date = date(dateTime)) %>%\n",
    "  mutate(lead_time = str_extract(str_extract(urls, \"f\\\\d+\"), \"\\\\d+\")) %>%\n",
    "  mutate(init_date_time = as_datetime(init_date)) \n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef2301",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Format URLs and COMID dataframes to allow for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db24f37",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "#### First, nest the URL dataframe\n",
    "\n",
    "all_urls_and_files_mt_nest <- all_urls_mt %>%\n",
    "  rename(predict_dateTime = dateTime) %>%\n",
    "  filter(lead_time <= 192) %>% ### Trim off the half-day at the end of each forecast\n",
    "  mutate(membr = member ) %>%\n",
    "  nest_by(init_date, membr) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(month = format(as.Date(init_date), \"%b%Y\"))\n",
    "\n",
    "#### Now, subset the files to download\n",
    "#### We do this so that we can iteratively save things because downloading \n",
    "#### the full years-long record would take forever\n",
    "#### This allows us to track our progress better \n",
    "\n",
    "#### Subset of files to download\n",
    "\n",
    "#################Change this to download month of interest######################\n",
    "\n",
    "trim_urls_and_files_mt_nest <- all_urls_and_files_mt_nest %>%\n",
    "  filter(month == \"Jun2024\" | month == \"Jul2024\") \n",
    "\n",
    "################################################################################\n",
    "\n",
    "#### Unnest the dataframe\n",
    "\n",
    "trim_urls_and_files_mt_unnest <- trim_urls_and_files_mt_nest %>%\n",
    "  unnest(data) %>%\n",
    "  dplyr::select(!membr) \n",
    "\n",
    "#### Declare where to write things \n",
    "\n",
    "write_file <- paste0(here(\"output-data/nwm_operational/medium_term\"), \"/\",\n",
    "                     trim_urls_and_files_mt_unnest$month[1],\n",
    "                     \".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6ce20",
   "metadata": {},
   "source": [
    "### Download NWM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578f862",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Extract the COMIDs that we are interested in\n",
    "#### Note that this inherits the selected_gages variable from\n",
    "#### 01_data_discovery.Rmd (so it requires that script to be run)\n",
    "\n",
    "stations <- lc_gages_metadata_clean %>%\n",
    "  .$comid %>%\n",
    "  as.numeric(.) \n",
    "\n",
    "\n",
    "#### Do the download\n",
    "#### Do it\n",
    "#### Note that this function tries a given URL five times maximum with a \n",
    "#### delay of 60 seconds between tries\n",
    "#### This allows minor blips in internet connection to not break the download\n",
    "#### If none of those tries returns actual data, it then returns a dataframe\n",
    "#### with NAs that is formatted (i.e., contains all the fields) of the actual\n",
    "#### data\n",
    "\n",
    "walk(finish_urls_and_files_mt_unnest$urls,\n",
    "     .f = possibly(insistently(get_timeseries3,\n",
    "                               rate = rate_delay(pause  = 60,\n",
    "                                                 max_times = 5)),\n",
    "                   otherwise = tibble(comid = NA, \n",
    "                                      init_date = NA, \n",
    "                                      lead_time = NA,\n",
    "                                      member = NA,\n",
    "                                      predict_dateTime = NA,\n",
    "                                      modeled_q_cms = NA)),\n",
    "     stations, ### Constant fed to .f \n",
    "     write_file,\n",
    "     .progress = TRUE)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f56e5",
   "metadata": {},
   "source": [
    "### Check to see if download completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011b9df",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Since we are downloading a month at a time, the download may break before\n",
    "#### The entire month is downloaded. To check, we want to see what the last date\n",
    "#### downloaded was\n",
    "#### If the entire month was downloaded it should be, for example, 2022-10-31\n",
    "\n",
    "### Read in the file\n",
    "nwm_forecasts <- read_csv(write_file)\n",
    "\n",
    "### See if we downloaded the whole month\n",
    "tail(nwm_forecasts)\n",
    "\n",
    "### Check the last date\n",
    "last_date <- last(nwm_forecasts)\n",
    "\n",
    "last_date\n",
    "\n",
    "### Find index of last timestep downloaded in the original df\n",
    "last_downloaded_index <- which(trim_urls_and_files_mt_unnest$init_date == \n",
    "                                 last_date$init_date &  \n",
    "                               trim_urls_and_files_mt_unnest$predict_dateTime == \n",
    "                                 last_date$predict_dateTime &\n",
    "                               trim_urls_and_files_mt_unnest$member ==\n",
    "                                 last_date$member)\n",
    "\n",
    "last_downloaded_index \n",
    "\n",
    "### Cut the dataframe down to last downloaded index \n",
    "### And to the end\n",
    "### To continue the download\n",
    "\n",
    "finish_urls_and_files_mt_unnest <- trim_urls_and_files_mt_unnest %>%\n",
    "  dplyr::slice(last_downloaded_index+1:n())\n",
    "\n",
    "\n",
    "### Check for missing\n",
    "missing_dates_indexes <- which(is.na(nwm_forecasts$predict_dateTime))\n",
    "\n",
    "missing_dates_indexes\n",
    "\n",
    "### If they exist, where are they\n",
    "\n",
    "missing_data <- nwm_forecasts %>%\n",
    "  mutate(date_mem = paste(init_date, member, sep = \"_\")) %>%\n",
    "\n",
    "unique(missing_data$date_mem)\n",
    "\n",
    "### Check all days are present\n",
    "unique(nwm_forecasts$init_date)\n",
    "\n",
    "unique(nwm_forecasts$member)\n",
    "\n",
    "### Check the time zone\n",
    "nwm_forecasts$predict_dateTime[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd442648",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,name,eval,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
